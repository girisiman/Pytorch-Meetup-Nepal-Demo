{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCTILFqrJPeF79dSdGtdFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girisiman/Pytorch-Meetup-Nepal-Demo/blob/main/quick_demo_colab_final_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° PyTorch Lightning - Interactive Demo\n",
        "\n",
        "**Instructions:**\n",
        "*   Run each cell to see Lightning in action - no prior setup needed!\n",
        "\n",
        "**What you'll learn:**\n",
        "1. How Lightning organizes PyTorch code\n",
        "2. One-line hardware scaling (CPU ‚Üí GPU ‚Üí Multi-GPU)\n",
        "3. Automatic best practices (logging, checkpointing, early stopping)\n",
        "4. Debugging superpowers\n"
      ],
      "metadata": {
        "id": "B40RyJxJ7KBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Part 1: Setup & Installation"
      ],
      "metadata": {
        "id": "QhQLejJ_7qfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install (if needed)\n",
        "!pip install torch torchvision lightning matplotlib\n",
        "\n",
        "# Cell 2: Show data\n",
        "print(\"This is CIFAR-10 data we'll train on...\")\n",
        "# Shows images"
      ],
      "metadata": {
        "id": "SlKG53eX6xYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Imports"
      ],
      "metadata": {
        "id": "RfNGUl4C7vxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Core\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# TorchVision for datasets\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# System\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Lightning version: {pl.__version__}\")"
      ],
      "metadata": {
        "id": "4pyMjHCl7t_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Part 2: Explore CIFAR-10 Dataset\n",
        "\n",
        "We'll use the CIFAR-10 dataset: 60,000; 32x32 color images in 10 classes"
      ],
      "metadata": {
        "id": "dL8VwHWs76jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "JmPffrxQ750q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "print(\"üì• Loading CIFAR-10 dataset...\")\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "1v9aFJVi8HsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset statistics\n",
        "print(f\"‚úÖ Training samples: {len(trainset):,}\")\n",
        "print(f\"‚úÖ Test samples: {len(testset):,}\")\n",
        "\n",
        "# Class names\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "print(f\"‚úÖ Classes: {classes}\")\n"
      ],
      "metadata": {
        "id": "i6vbIUge8J-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize some samples from the dataset\n",
        "\n",
        "def show_images(images, labels, n=4):\n",
        "    \"\"\"Display n images with their labels\"\"\"\n",
        "    fig, axes = plt.subplots(1, n, figsize=(15, 4))\n",
        "    for i in range(n):\n",
        "        # Unnormalize the image\n",
        "        img = images[i] / 2 + 0.5  # Scale from [-1, 1] to [0, 1]\n",
        "        axes[i].imshow(img.permute(1, 2, 0))  # CHW ‚Üí HWC\n",
        "        axes[i].set_title(f\"Label: {classes[labels[i]]}\")\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oVlBSo2Z8MfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "trainloader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "print(\"üñºÔ∏è Sample images from CIFAR-10:\")\n",
        "show_images(images, labels, n=4)"
      ],
      "metadata": {
        "id": "B40oi_ya8PXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Part 3: Create a LightningModule\n",
        "\n",
        "**Key Concept:** LightningModule organizes your PyTorch code into 6 logical sections:\n",
        "  1. Initialization (`__init__`)\n",
        "  2. Training loop (`training_step`)\n",
        "  3. Validation loop (`validation_step`)\n",
        "  4. Test loop (`test_step`)\n",
        "  5. Prediction loop (`predict_step`)\n",
        "  6. Optimizers (`configure_optimizers`)"
      ],
      "metadata": {
        "id": "T8n3AacO8Wb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create a pure PyTorch model (no Lightning here!)\n",
        "# This is the model architecture you would write anyway\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"A simple CNN for CIFAR-10 classification - Pure PyTorch!\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # 3 input channels (RGB), 32 output\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Reduces spatial dimensions by half\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # After 2 pooling layers: 32x32 ‚Üí 16x16 ‚Üí 8x8\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass - define how data flows through the network\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv ‚Üí ReLU ‚Üí Pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Another conv block\n",
        "\n",
        "        x = torch.flatten(x, 1)  # Flatten for fully connected layers\n",
        "        x = F.relu(self.fc1(x))  # Fully connected with ReLU\n",
        "        x = self.dropout(x)      # Regularization\n",
        "        x = self.fc2(x)          # Output layer (no activation - softmax in loss)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "se_z9Mm-8R-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our pure PyTorch model\n",
        "print(\"Testing pure PyTorch model...\")\n",
        "sample_input = torch.randn(4, 3, 32, 32)  # Batch of 4, 3 channels, 32x32 images\n",
        "model = SimpleCNN()\n",
        "output = model(sample_input)\n",
        "print(f\"Input shape: {sample_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
      ],
      "metadata": {
        "id": "meW0EF3I8q1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now wrap it in a LightningModule\n",
        "**Magic happens here:** LightningModule takes your PyTorch model and adds training logic"
      ],
      "metadata": {
        "id": "bAF7U8DM8zKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitCIFAR(pl.LightningModule):\n",
        "    \"\"\"LightningModule that wraps our PyTorch model with training logic\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save hyperparameters (auto-logged!)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Use our pure PyTorch model - no changes needed!\n",
        "        self.model = SimpleCNN()\n",
        "\n",
        "        # Loss function\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Track metrics\n",
        "        from torchmetrics import Accuracy # Import Accuracy from torchmetrics\n",
        "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
        "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
        "\n",
        "    # ========== 1. Forward Pass ==========\n",
        "    def forward(self, x):\n",
        "        \"\"\"Inference - same as PyTorch forward\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "    # ========== 2. Training Step ==========\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"What happens in ONE training batch\"\"\"\n",
        "        x, y = batch\n",
        "\n",
        "        # Forward pass\n",
        "        y_hat = self(x)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        self.train_acc(preds, y)\n",
        "\n",
        "        # Log everything - automatically sent to TensorBoard, etc.\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', self.train_acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # ========== 3. Validation Step ==========\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"What happens in ONE validation batch\"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        self.val_acc(preds, y)\n",
        "\n",
        "        # Log validation metrics\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', self.val_acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # ========== 4. Test Step ==========\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"What happens in ONE test batch\"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        # Compute test accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('test_acc', acc)\n",
        "\n",
        "        return acc\n",
        "\n",
        "    # ========== 5. Configure Optimizers ==========\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Define optimizers and learning rate schedulers\"\"\"\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "\n",
        "        # Add a learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'val_loss'  # Which metric to monitor for scheduling\n",
        "        }\n",
        "\n",
        "print(\"LightningModule created!\")\n",
        "print(\"Notice: We separated the WHAT (model, loss) from the HOW (training loop)\")"
      ],
      "metadata": {
        "id": "Z2S15vG48tB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Part 4: Create a LightningDataModule\n",
        "\n",
        "**Key Concept:** DataModule organizes your data loading code for reproducibility"
      ],
      "metadata": {
        "id": "llSiA2Gw9FS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    \"\"\"Handles all data loading and preprocessing for CIFAR-10\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Download data - runs only once on 1 GPU\"\"\"\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Split data - runs on every GPU\"\"\"\n",
        "        if stage == 'fit' or stage is None:\n",
        "            # Load full training set\n",
        "            full_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform\n",
        "            )\n",
        "            # Split into train/validation (45k/5k)\n",
        "            self.train_data, self.val_data = random_split(full_train, [45000, 5000])\n",
        "            print(f\"Train: {len(self.train_data):,} samples\")\n",
        "            print(f\"Validation: {len(self.val_data):,} samples\")\n",
        "\n",
        "        if stage == 'test' or stage is None:\n",
        "            # Test set\n",
        "            self.test_data = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform\n",
        "            )\n",
        "            print(f\"Test: {len(self.test_data):,} samples\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size,\n",
        "                         shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=self.batch_size,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_data, batch_size=self.batch_size,\n",
        "                         shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "OfWXCzE18-lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and setup our data module\n",
        "print(\"Creating DataModule...\")\n",
        "data_module = CIFAR10DataModule(batch_size=64)\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "# Show data loader statistics\n",
        "print(f\"\\n DataLoader statistics:\")\n",
        "print(f\"Train batches: {len(data_module.train_dataloader()):,}\")\n",
        "print(f\"Val batches: {len(data_module.val_dataloader()):,}\")\n",
        "print(f\"Test batches: {len(data_module.test_dataloader()):,}\")"
      ],
      "metadata": {
        "id": "HgkO4rBe9OWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Part 5: The Trainer - Lightning's Superpower\n",
        "\n",
        "**This is where the magic happens!**  \n",
        "\n",
        "### The `Trainer` class automates everything you used to write manually:\n",
        "Training loop\n",
        "- Validation loop\n",
        "- Checkpointing\n",
        "- Logging\n",
        "- Multi-GPU training\n",
        "- Early stopping\n",
        "- And much more!"
      ],
      "metadata": {
        "id": "Qr0cCIDS9jUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our model\n",
        "model = LitCIFAR(learning_rate=1e-3)"
      ],
      "metadata": {
        "id": "8O_78PlM9Srx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Fast Development Run\n",
        "\n",
        "**Problem:** In PyTorch, you write the whole training loop before you can test if it works.  \n",
        "**Solution:** `fast_dev_run=True` runs 1 batch to verify everything works!"
      ],
      "metadata": {
        "id": "axgad_a_-Ias"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DEMO 1: Fast Development Run\")\n",
        "print(\"=\" * 50)\n",
        "trainer_dev = Trainer(\n",
        "    fast_dev_run=True,      # Runs only 1 batch\n",
        "    max_epochs=1,           # Just 1 epoch\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=True,\n",
        ")\n",
        "\n",
        "print(\"\\nRunning fast_dev_run (should take < 10 seconds)...\")\n",
        "trainer_dev.fit(model, data_module)\n",
        "\n",
        "print(\"\\n Success! Our pipeline works.\")\n",
        "print(\"   No need to write training loops just to test if code works!\")"
      ],
      "metadata": {
        "id": "w816GQFA9wtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2: CPU Training (Baseline)\n"
      ],
      "metadata": {
        "id": "Rb33MgFH-YcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n DEMO 2: CPU Training\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "mBtrzdOk_DLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_cpu = Trainer(\n",
        "    accelerator='cpu',      # Force CPU\n",
        "    max_epochs=1,           # Just 1 epoch for demo\n",
        "    log_every_n_steps=20,   # Log every 20 batches\n",
        "    enable_progress_bar=True,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining on CPU...\")"
      ],
      "metadata": {
        "id": "pLInzrI_-S8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to run:\n",
        "trainer_cpu.fit(model, data_module)"
      ],
      "metadata": {
        "id": "aHYlCWly-jZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Same code runs on CPU with zero changes!\")"
      ],
      "metadata": {
        "id": "Y95va1gq-nI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 3: GPU Training (Automatic Detection)"
      ],
      "metadata": {
        "id": "Y8U46mdD-7di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n DEMO 3: GPU Training (Auto-detect)\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "KiXTPGe9-rIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what hardware is available\n",
        "print(f\"Hardware check:\")\n",
        "print(f\"   CPU cores: Available\")\n",
        "print(f\"   GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"   GPU name: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "metadata": {
        "id": "dKSxMxYx--x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_gpu = Trainer(\n",
        "    accelerator='auto',     # Automatically picks GPU if available\n",
        "    devices='auto',         # Use all available devices\n",
        "    max_epochs=1,\n",
        "    log_every_n_steps=20,\n",
        "    enable_progress_bar=True,\n",
        ")"
      ],
      "metadata": {
        "id": "DjlT44rz_Kg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Key insight: Change 'cpu' to 'auto' to use GPU\")\n",
        "print(\"   No code changes in model or data!\")"
      ],
      "metadata": {
        "id": "nSqK8vwo_NxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to run if GPU is available:\n",
        "if torch.cuda.is_available():\n",
        "  trainer_gpu.fit(model, data_module)\n",
        "else:\n",
        "  print(\"‚ö†Ô∏è  No GPU available, but code would work if there was!\")\n"
      ],
      "metadata": {
        "id": "HitJmXet_Q5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 4: Mixed Precision Training (2x Speed!)"
      ],
      "metadata": {
        "id": "fSVXu4sT_eLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n DEMO 4: Mixed Precision Training\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "jTw3_Nwh_ZJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_mixed = Trainer(\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    precision=16,           # Mixed precision - 2x faster!\n",
        "    max_epochs=1,\n",
        "    log_every_n_steps=20,\n",
        ")"
      ],
      "metadata": {
        "id": "XN_pQw7G_lrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Change: Add precision=16 for automatic mixed precision\")\n",
        "print(\"   Benefits: 2x faster training, half the memory usage\")\n",
        "print(\"   No code changes to your model!\")"
      ],
      "metadata": {
        "id": "tm7SYlAb_qOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 5: Multi-GPU Training\n"
      ],
      "metadata": {
        "id": "jKurTqPJ_xhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n DEMO 5: Multi-GPU Training\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "6KVHoYfM_1-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
        "    trainer_multi = Trainer(\n",
        "        accelerator='gpu',\n",
        "        devices=2,              # Use 2 GPUs\n",
        "        strategy='ddp',         # Distributed Data Parallel\n",
        "        max_epochs=1,\n",
        "    )\n",
        "    print(\"‚úÖ Code ready for 2-GPU training!\")\n",
        "    print(\"   Change: devices=1 ‚Üí devices=2\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Only {torch.cuda.device_count() if torch.cuda.is_available() else 0} GPU(s) available\")\n",
        "    print(\"   Code would work with 2+ GPUs!\")\n"
      ],
      "metadata": {
        "id": "BnODBhpX_7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 6: With Callbacks (Production Ready)"
      ],
      "metadata": {
        "id": "X4hgijyg_-XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n DEMO 6: Production Training with Callbacks\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "KGbNjtaHADOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create useful callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    filename='cifar10-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=2,\n",
        "    mode='min',\n",
        "    save_last=True,\n",
        ")\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    mode='min',\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "trainer_prod = Trainer(\n",
        "    accelerator='auto',\n",
        "    max_epochs=10,\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        "    log_every_n_steps=50,\n",
        "    gradient_clip_val=1.0,      # Prevent exploding gradients\n",
        "    accumulate_grad_batches=2,  # Simulate larger batch size\n",
        "    deterministic=True,         # Reproducible results\n",
        ")"
      ],
      "metadata": {
        "id": "E0GEqU_QAG0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Added production features with callbacks:\")\n",
        "print(\"   ‚Ä¢ Automatic checkpointing (saves best models)\")\n",
        "print(\"   ‚Ä¢ Early stopping (prevents overfitting)\")\n",
        "print(\"   ‚Ä¢ Gradient clipping (stabilizes training)\")\n",
        "print(\"   ‚Ä¢ Gradient accumulation (larger effective batch size)\")\n",
        "print(\"   ‚Ä¢ Deterministic training (reproducible)\")"
      ],
      "metadata": {
        "id": "1dCp0inLAKdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiements with different Learning Rates"
      ],
      "metadata": {
        "id": "PHoSupsBAju9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models with different learning rates\n",
        "print(\"\\nCreating models with different learning rates...\")"
      ],
      "metadata": {
        "id": "DKCUNJgBApYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr_low = LitCIFAR(learning_rate=1e-4)   # Very low LR\n",
        "model_lr_med = LitCIFAR(learning_rate=1e-3)   # Medium LR (default)\n",
        "model_lr_high = LitCIFAR(learning_rate=1e-2)  # High LR"
      ],
      "metadata": {
        "id": "vYNZbBXdAvGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer for experiments - FORCE CPU\n",
        "trainer_experiment = Trainer(\n",
        "    accelerator='cpu',          # Force CPU for consistent results\n",
        "    fast_dev_run=True,          # Only 1 batch per experiment\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=False, # Cleaner output\n",
        ")"
      ],
      "metadata": {
        "id": "ip37YiNCAvyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Testing different learning rates on CPU:\")\n",
        "print(\"   1. LR = 0.0001 (very low)\")\n",
        "print(\"   2. LR = 0.001  (medium - default)\")\n",
        "print(\"   3. LR = 0.01   (high)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ],
      "metadata": {
        "id": "RuTwjRT5BVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment 1: Very low learning rate\n",
        "print(\"\\n Testing LR = 0.0001 (very low)...\")\n",
        "trainer_experiment.fit(model_lr_low, data_module)\n",
        "\n",
        "# Reset data module for next experiment\n",
        "data_module.setup()\n"
      ],
      "metadata": {
        "id": "IukAemeBBahf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment 2: Medium learning rate\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"\\n Testing LR = 0.001 (medium - default)...\")\n",
        "trainer_experiment.fit(model_lr_med, data_module)\n"
      ],
      "metadata": {
        "id": "QPwr6lBlBg5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset data module for next experiment\n",
        "data_module.setup()\n",
        "# Run experiment 3: High learning rate\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"\\n Testing LR = 0.01 (high)...\")\n",
        "trainer_experiment.fit(model_lr_high, data_module)"
      ],
      "metadata": {
        "id": "xAkwbSAdBnnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\" Learning rate experiments complete!\")\n",
        "print(\"\\n Observations:\")\n",
        "print(\"   ‚Ä¢ Very low LR (0.0001): May converge slowly\")\n",
        "print(\"   ‚Ä¢ Medium LR (0.001): Good balance for most models\")\n",
        "print(\"   ‚Ä¢ High LR (0.01): May cause instability\")\n",
        "print(\"\\n Tip: Use fast_dev_run to quickly test hyperparameters!\")"
      ],
      "metadata": {
        "id": "V6HvRaRCBt9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Analysis\n"
      ],
      "metadata": {
        "id": "f4ZLeHHuB4qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Learning Rate Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Let's run a slightly longer experiment to see the difference\n",
        "print(\"\\nRunning longer experiment (3 batches) to see trends...\")\n",
        "\n",
        "# Reset models\n",
        "models = {\n",
        "    \"Low LR (1e-4)\": LitCIFAR(learning_rate=1e-4),\n",
        "    \"Medium LR (1e-3)\": LitCIFAR(learning_rate=1e-3),\n",
        "    \"High LR (1e-2)\": LitCIFAR(learning_rate=1e-2)\n",
        "}"
      ],
      "metadata": {
        "id": "jDyrb7-1A8Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset models\n",
        "models = {\n",
        "    \"Low LR (1e-4)\": LitCIFAR(learning_rate=1e-4),\n",
        "    \"Medium LR (1e-3)\": LitCIFAR(learning_rate=1e-3),\n",
        "    \"High LR (1e-2)\": LitCIFAR(learning_rate=1e-2)\n",
        "}\n",
        "\n",
        "trainer_analysis = Trainer(\n",
        "    accelerator='cpu',\n",
        "    max_epochs=1,\n",
        "    limit_train_batches=3,  # 3 batches to see trend\n",
        "    limit_val_batches=2,    # 2 validation batches\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=False,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining each model for 3 batches...\")\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "9AlrCWvWB-z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n Training: {name}\")\n",
        "    trainer_analysis.fit(model, data_module)\n",
        "\n",
        "    # Store the final loss for comparison\n",
        "    final_loss = trainer_analysis.callback_metrics.get('train_loss', torch.tensor(0.0))\n",
        "    if isinstance(final_loss, torch.Tensor):\n",
        "        final_loss = final_loss.item()\n",
        "    results[name] = final_loss\n",
        "\n",
        "    # Reset data module for next model\n",
        "    data_module.setup()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\" RESULTS SUMMARY:\")\n",
        "print(\"-\" * 50)\n",
        "for name, loss in results.items():\n",
        "    print(f\"{name:20s} ‚Üí Final Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"\\n Key Insights:\")\n",
        "print(\"1. Higher learning rates converge faster initially\")\n",
        "print(\"2. Very low learning rates need more time to converge\")\n",
        "print(\"3. Medium LR (1e-3) is usually a good starting point\")\n",
        "print(\"4. Use fast_dev_run to quickly test different LRs!\")"
      ],
      "metadata": {
        "id": "LjDu4f3UCCUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Visualization (CPU)"
      ],
      "metadata": {
        "id": "VIBRfjf3C9pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fresh model for visualization\n",
        "model_viz = LitCIFAR(learning_rate=1e-3)"
      ],
      "metadata": {
        "id": "opc0PmWQCFaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for visualization - ON CPU\n",
        "trainer_viz = Trainer(\n",
        "    accelerator='cpu',           # Explicitly use CPU\n",
        "    max_epochs=2,               # 2 epochs for better visualization\n",
        "    limit_train_batches=50,      # 50 batches for meaningful trend\n",
        "    limit_val_batches=20,        # 20 validation batches\n",
        "    log_every_n_steps=10,        # Log every 10 steps\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=True,   # Show model summary\n",
        "    callbacks=[],               # No callbacks for cleaner output\n",
        ")\n",
        "\n",
        "print(\"\\n Starting CPU training for visualization...\")\n",
        "print(\"   Training for 2 epochs (50 batches per epoch)\")\n",
        "print(\"   Watch the progress bar and metrics update!\")\n",
        "print(\"\\n\" + \"=\" * 50)"
      ],
      "metadata": {
        "id": "nFtJxGW0CT6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = trainer_viz.fit(model_viz, data_module)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CPU Training Complete!\")\n",
        "print(\"\\n What just happened automatically:\")\n",
        "print(\"   ‚úì Training loop executed\")\n",
        "print(\"   ‚úì Validation after each epoch\")\n",
        "print(\"   ‚úì Loss and accuracy logged\")\n",
        "print(\"   ‚úì Progress tracked with ETA\")\n",
        "print(\"   ‚úì All on CPU with zero GPU code!\")"
      ],
      "metadata": {
        "id": "bPziKc_TCV0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Check the metrics above:\")\n",
        "print(\"   ‚Ä¢ train_loss: Decreased over time (good!)\")\n",
        "print(\"   ‚Ä¢ train_acc: Increased (learning happening!)\")\n",
        "print(\"   ‚Ä¢ val_loss/val_acc: Model generalizing\")\n",
        "\n",
        "print(\"\\n Try changing these and re-running:\")\n",
        "print(\"   ‚Ä¢ max_epochs=2 ‚Üí max_epochs=5\")\n",
        "print(\"   ‚Ä¢ learning_rate=1e-3 ‚Üí learning_rate=1e-2\")\n",
        "print(\"   ‚Ä¢ Add EarlyStopping callback\")"
      ],
      "metadata": {
        "id": "Hk5LsTAUCa-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}